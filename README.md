# Adversarial Attack on Image Data using PyTorch

## About the Project

Adversarial Attack is the methodology used to trick deep neural networks to misclassify data by slightly perturbing the original data. These perturbations are so small that they aren't even visible to human eye.

In this project I implement three types of attacks on MNIST dataset which are implemented based on the following papers:

1. FGSM - [Link to paper](https://arxiv.org/abs/1412.6572)
2. Deepfool Attack - [Link to paper](https://arxiv.org/abs/1511.04599)

Results:

![image1][images/fgsm_example.png]
![image2][images/lbfgs_example.png]
![image3][images/mnist example.png]
